{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "The Data Preprocessing is a technique to transform the raw data into a cleaned data before feeding it to the algorithms. This gives a significant amount of improvements to the model and can be evaluated oon the metrics and the improvement is also dependent on the dataset on which preprocessing is been applied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several techniques used for the data preprocessing:\n",
    "# 1. Rescaling :\n",
    "When a data has different features with different scale then this could lead to significant amount of error. Many times the applied algorithms can be benefitted with the rescaled data. Rescaling the data allows datasset to be on same scale and be in between defined upper and lower bounds. This is useful for the weighted inputs used mainly in the regression and algorithms like k nearest neighbour that uses distance measures for evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edit before running\n",
    "# Python code to Rescale data (between 0 and 1) \n",
    "\n",
    "import pandas \n",
    "import scipy \n",
    "import numpy \n",
    "from sklearn.preprocessing import MinMaxScaler \n",
    "\n",
    "dataframe = pandas.read_csv(\"enter csv file of dataset\") \n",
    "array = dataframe.values \n",
    "  \n",
    "# separate array into input and output components \n",
    "X =  # input features\n",
    "Y = # target feature in case of regression\n",
    "scaler = MinMaxScaler(feature_range=(0, 1)) \n",
    "rescaledX = scaler.fit_transform(X) \n",
    "  \n",
    "# summarize transformed data \n",
    "numpy.set_printoptions(precision=3) \n",
    "print(rescaledX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output be like the cell below: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample output\n",
    "[[ 0.353  0.744  0.59   0.354  0.0    0.501  0.234  0.483]\n",
    " [ 0.059  0.427  0.541  0.293  0.0    0.396  0.117  0.167]\n",
    " [ 0.471  0.92   0.525  0.     0.0    0.347  0.254  0.183]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Standardize Data:\n",
    "â€¢ Standardization is a useful technique to transform attributes with a Gaussian distribution and differing means and standard deviations to a standard Gaussian distribution with a mean of 0 and a standard deviation of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edit before running\n",
    "# Python code to Standardize data (0 mean, 1 stdev) \n",
    "\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "import pandas \n",
    "import scipy \n",
    "import numpy  \n",
    "\n",
    "dataframe = pandas.read_csv(\"enter csv file of dataset\") \n",
    "array = dataframe.values \n",
    "  \n",
    "# separate array into input and output components \n",
    "X =  # input features\n",
    "Y = # target feature in case of regression\n",
    "scaler = StandardScaler().fit(X) \n",
    "rescaledX = scaler.transform(X) \n",
    "  \n",
    "# summarize transformed data \n",
    "numpy.set_printoptions(precision=3) \n",
    "print(rescaledX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output be like:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[1.944 -0.264 -1.288 -0.693 -1.103  0.604 -0.106]\n",
    " [-0.845 -0.998 -0.161  0.155  0.123 -0.494 -0.921 -1.042]\n",
    " [-1.142  0.504 -1.505  0.907  0.766  1.41 ]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Imputation:\n",
    "This technique is used to deal with the missing values.Imputation fills in the missing value with some number(it is complicated). The imputed value won't be exactly right in most cases, but it usually gives more accurate models than dropping the column entirely.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputation template\n",
    "# edit required fields\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Importing the dataset\n",
    "dataset = pd.read_csv('DataFile.csv')\n",
    "X = 'input feature column'\n",
    "y = target feature colums\n",
    "\n",
    "# Taking care of missing data\n",
    "from sklearn.preprocessing import Imputer\n",
    "imputer = Imputer(missing_values = 'NaN', strategy = 'mean', axis = 0)\n",
    "imputer = imputer.fit(X[:, 1:3])\n",
    "X[:, 1:3] = imputer.transform(X[:, 1:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Hot encoding:\n",
    "This is to deal with the categorical data (data columns with the datatype = object) as many machine learning algorithms do not work efficiently on catagorical data. One hot encoding creates new (binary) columns, indicating the presence of each possible value from the original data. THis is helpful  unless categorical data takes on a large number of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hot encoding  template\n",
    "# edit required fields\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "# Importing the dataset\n",
    "dataset = pd.read_csv('DataFile.csv')\n",
    "X = 'input feature column'\n",
    "y = target feature colums\n",
    "\n",
    "\n",
    "labelencoder_X = LabelEncoder()\n",
    "X = labelencoder_X.fit_transform(X[:, 0])\n",
    "onehotencoder = OneHotEncoder(categorical_features = [0])\n",
    "X = onehotencoder.fit_transform(X).toarray()\n",
    "\n",
    "# Encoding the Dependent Variable\n",
    "labelencoder_y = LabelEncoder()\n",
    "y = labelencoder_y.fit_transform(y)\n",
    "print(X,y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
